# Regression

Load data sets of this chapter

```{r}
data("apm", package = "DataRZ")
data("unique2010", package = "DataRZ")
data("baby", package = "DataRZ")

data("iris")
data("Leinhardt", package = "carData")
data("Prestige", package = "carData")
data("gala", package = "faraway")
```

## Preperation

##### Plot predictors {-}

```{r}
library(FunRZ)
pairs(iris, lower.panel = panel.smooth, diag.panel = panel.hist,
      upper.panel = panel.cor)
```

##### Transform and check for NA {-}

```{r, eval=FALSE}
any(is.na(iris$Sepal.length))
iris$log_sepal <- log(iris$Sepal.length)
```


## Fit

* `lm()` fit linear model
* Extract values
    - `residuals()` raw residuals
    - `rstandard()` standardized residuals
    - `rstudent()` studentized residuals
    - `sigma()` residual standard deviation $\sigma_E$ (alternative `summary(fit)$sigma`)

## Inference

* `summary()` estimates, std. error, p values, multiple R-squared
* `confint()` CI

```{r}
fit <- lm(Pax ~ ATM, data = unique2010)
```

Automatic testing $H_0: \beta_j = 0$
```{r}
summary(fit)
```

Manual testing $H_0: \beta_j = b$
```{r, eval=FALSE}
b <- 5  # for example
mycoef <- summary(fit)$coefficients
t_val <- (mycoef["(Intercept)","Estimate"]-b)/mycoef["(Intercept)","Std. Error"]
(1-pt(abs(t_val), df))*2    ## df = degrees of freedom of residuals (find in summary)
                            ## *2 two sided test
```

Automatic CI
```{r, eval=FALSE}
confint(fit, "ATM")
confint(fit, "(Intercept)")
```

Manual CI
```{r, eval=FALSE}
mycoef <- summary(fit)$coefficients
mycoef["(Intercept)","Estimate"] + qt(c(0.025,0.975), df)*mycoef["(Intercept)","Std. Error"] ## df = degrees of freedom of residuals
```

## Prediction

* `fitted()` fitted values
* `predict()` predict for any predictor value
    - `newdata` needs to be a `data.frame` with same colomn name as predictors

```{r}
## Predict values for any predictor values
dat <- data.frame(ATM=c(24000))
predict(fit, newdata=dat)

## CI for regression line
dat <- data.frame(ATM=seq(18000, 26000, length=200))
ci <- predict(fit, newdata=dat, interval="confidence")

## PI for regression line
dat <- data.frame(ATM=seq(18000, 26000, length=200))
pi <- predict(fit, newdata=dat, interval="prediction")
```




## Plot regression

Include regression formula

```{r, out.width='50%'}
set.seed(1)
x <- rnorm(50, mean = 10, sd = 2)
y <- x + rnorm(50, sd = 2)
plot(y ~ x)
fit <- lm(y ~ x)
abline(fit)
x_cor <- grconvertX(0.1, from = 'npc')
y_cor <- grconvertY(0.9, from = 'npc')
fit_eq <- format(coef(fit), digits = 2)
fit_r_sq <- format(summary(fit)$adj.r.squared, digits = 2)
fit_cor <- format(cor(x = x, y = y), digits = 2)

text(x = x_cor, y = y_cor, pos = 4,
     labels = bquote(y == .(fit_eq[2]) * x + .(fit_eq[1])))
text(x = x_cor, y = y_cor-1, pos = 4,
     labels = bquote(R[adj]^2 == .(fit_r_sq)))
text(x = x_cor, y = y_cor-2, pos = 4,
     labels = bquote(cor == .(fit_cor)))
```

Include CI and PI

```{r}
fit <- lm(Pax ~ ATM, data = unique2010)
dat <- data.frame(ATM=seq(18000, 26000, length=200))
ci <- predict(fit, newdata=dat, interval="confidence")
pi <- predict(fit, newdata=dat, interval="prediction")
plot(Pax ~ ATM, data=unique2010, pch=20)

lines(dat$ATM, ci[,2], col="green")
lines(dat$ATM, ci[,3], col="green")
lines(dat$ATM, pi[,2], col="blue")
lines(dat$ATM, pi[,3], col="blue")
abline(fit, col="red", lwd=2)
```

Regression line with log transformation


```{r}
fit <- lm(log(infant) ~ log(income), data = Leinhardt)
plot(infant ~ income, data = Leinhardt)
my_pre <- data.frame(income = seq(min(Leinhardt$income), max(Leinhardt$income), length.out = 200))
my_pre$infant <- exp(predict(fit, newdata = my_pre))
lines(my_pre$income, my_pre$infant)
```


## Model comparison

* Gobal F-test `summary()`
* Partial F-test
    - `anova()`
        + perfect but both models need to be specified
        + significance means evidence against the simpler model
    - `drop1()`
        + drops only single terms
        + correctly handles factors/interactions
            * no testing of single factor levels (factor as a whole is tested)
            * hirarchical structure is considered (first drop interaction before droping main factor)
    - `summary()`
        + drops only single parameters
        + wrong handling of factors/interactions


## Diagnostic plots

```{r,echo=FALSE}
library(FunRZ)

# cross_check function ----------------------------------------------------
cross_check <- function(check = TRUE){
  if(check){
    text(x = grconvertX(0.05, from = "npc"),
         y = grconvertY(0.99, from = "npc"),
         labels = "\U2713",
         cex = 4,
         adj = c(0,1),
         col = "lightgreen")
  } else {
    text(x = grconvertX(0.05, from = "npc"),
         y = grconvertY(0.99, from = "npc"),
         labels = "x",
         cex = 4,
         adj = c(0,1),
         col = "red")
  }
}
```


### Tukey-Anscombe plot
```{r, echo=FALSE, out.width = '50%', fig.show='hold', fig.align='default'}
set.seed(1)
x <- 1:100
y <- rnorm(100, x + 10, 10)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 1, change_mfrow = F, plot_title = F)
title("Gaussian iid Residuals")
cross_check(T)

set.seed(3)
x <- 1:100
y <- rnorm(100, x + 10, 10)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 1, change_mfrow = F, plot_title = F)
title("Gaussian iid Residuals")
cross_check(T)

set.seed(10)
x <- 1:100
y <- rnorm(100, x + 10, x)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 1, change_mfrow = F, plot_title = F)
title("Heteroskedasticity")
cross_check(F)

set.seed(1)
x <- 1:100
y <- rnorm(100, x^1.3, 10)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 1, change_mfrow = F, plot_title = F)
title("Systematic Error")
cross_check(F)
```

```{r, eval=FALSE}
# Automatic
plot(fit, which = 1)

# Manual
plot(fitted(fit), residuals(fit))
abline(h = 0)
lines(loess.smooth(fitted(fit), residuals(fit)),
      col = "red")

# Resampling
FunRZ::FunResplot(fit, plot = 1)

```


### Normal QQ plot
```{r, echo=FALSE, out.width = '50%', fig.show='hold', fig.align='default'}
set.seed(1)
x <- 1:100
y <- rnorm(100, x + 10, 10)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 2, change_mfrow = F, plot_title = F)
title("Normal")
cross_check(T)

set.seed(1)
x <- 1:100
y <- 10 + x + rexp(100)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 2, change_mfrow = F, plot_title = F)
title("Right-Skewed")
cross_check(F)

set.seed(9)
x <- 1:100
y <- 10 + x + rnorm(100)^3
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 2, change_mfrow = F, plot_title = F)
title("Long-Tailed")
cross_check(F)

set.seed(2)
x <- 1:100
y <- 10 + x + sqrt(abs(rnorm(100)))*c(-1,1)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 2, change_mfrow = F, plot_title = F)
title("Short-Tailed")
cross_check(T)
```

```{r, eval=FALSE}
# Automatic
plot(fit, which = 2)

# Manual
qqnorm(rstandard(fit))
qqline(rstandard(fit))

# Resampling
FunRZ::FunResplot(fit, plot = 2)

```


### Scale-Location plot
```{r, echo=FALSE, out.width = '50%', fig.show='hold', fig.align='default'}
set.seed(10)
x <- 1:100
y <- rnorm(100, x + 10, 10)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 3, change_mfrow = F, plot_title = F)
title("Homoskedasticity")
cross_check(T)

set.seed(10)
x <- 1:100
y <- rnorm(100, x + 10, x)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 3, change_mfrow = F, plot_title = F)
title("Heteroskedasticity")
cross_check(F)
```

```{r, eval=FALSE}
# Automatic
plot(fit, which = 3)

# Manual
res <- sqrt(abs((rstandard(fit))))
plot(fitted(fit), res)
lines(loess.smooth(fitted(fit), res),
      col="red")

# Resampling
FunRZ::FunResplot(fit, plot = 3)
```


### Leverage plot
```{r, echo=FALSE, out.width = '50%', fig.show='hold', fig.align='default'}
set.seed(10)
x <- 1:100
y <- rnorm(100, x + 10, 10)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 4, change_mfrow = F, plot_title = F)
title("No Outliers")
cross_check(T)

set.seed(10)
x <- c(1:100, 130)
y <- c(rnorm(100, x + 10, 10), 100)
mymod <- lm(y ~ x)
FunResplot(mymod, plots = 4, change_mfrow = F, plot_title = F)
title("Outlier with Leverage")
cross_check(F)
```

```{r, eval=FALSE}
# Automatic
plot(fit, which = 4)
```


### Residuals vs any
```{r, echo=FALSE, out.width = '50%', fig.show='hold', fig.align='default'}
set.seed(2)
plot(1:30, rnorm(30), pch = 20, ylab = "residuals", xlab = "any variable (e.g. time)")
abline(h = 0, lty = 2)
title("Independant")
cross_check(T)

set.seed(2)
plot(1:30, rnorm(30)+10*sin(c(1:30)*0.2),
     pch = 20, ylab = "residuals", xlab = "any variable, (e.g. time)")
abline(h = 0, lty = 2)
title("Autocorrelated")
cross_check(F)
```

```{r, eval=FALSE}
# vs Predictors
plot(residuals(fit) ~ ., data = df)

# vs Possible predictors
plot(residuals(fit) ~ ., data = df)

# vs Temporal/Spatial information
plot(residuals(fit))  ## against Index
plot(residuals(fit) ~ Time-Space-Variable)
```


### Partial residual plot
```{r, echo=FALSE, out.width = '50%', fig.show='hold', fig.align='default'}
## data
set.seed(1)
x1 <- 1:100
x2 <- rnorm(100, 3, 1)

############## good example (linear)
y <- x1 - x2*10 + rnorm(100, 0, 2)
fit <- lm(y~x1 + x2)
myres <- residuals(fit, type = "partial")
mycof <- fit$coefficients

plot(myres[,"x1"] ~ x1,
     ylab = "partial resiudals of x1")
title("Linear relationship")
abline(a = -mycof["x1"]*mean(x1), b = mycof["x1"])
lines(loess.smooth(x1,myres[,"x1"]), col = "red")

############## bad example (non-linear)
y <- sqrt(x1)*10 - x2*10 + rnorm(100, 0, 2)
fit <- lm(y~x1 + x2)
myres <- residuals(fit, type = "partial")
mycof <- fit$coefficients

plot(myres[,"x1"] ~ x1,
     ylab = "partial resiudals of x1")
title("Non-linear relationship")
abline(a = -mycof["x1"]*mean(x1), b = mycof["x1"])
lines(loess.smooth(x1,myres[,"x1"]), col = "red")
```

```{r, eval=FALSE}
# Automatic
car::crPlots(fit)
car::crPlot(fit, variable = x1)

# Manual
myres <- residuals(fit, type = "partial")
mycof <- fit$coefficients
## for target variable x1
plot(myres[,"x1"] ~ x1)
abline(a = -mycof["x1"]*mean(x1), b = mycof["x1"])
lines(loess.smooth(x1, myres[,"x1"]), col = "red")
```



## Model improvement

### Multicollinearity

The variance inflation factor $\text{VIF}$ quantifies the multicollinearity between predictor variables.

* Fit contains only numerical predictors: $\text{VIF} > 5$ critically
* Fit contains also factorial predictors: `GVIF^(1/(2*Df))` $> 5$ critically

```{r, collapse=TRUE}
## Only numerical predictors
fit <- lm(Pax ~ ATM + Cargo, data = unique2010)
car::vif(fit)

## Contains fatorial predictor
fit <- lm(Sepal.Length ~ Sepal.Width + Species, data = iris)
car::vif(fit)
```

### AIC/BIC

\begin{align*}
\text{AIC} &= -2\log(\text{L}) + 2p \\
\text{BIC} &= -2\log(\text{L}) + \log(n)p
\end{align*}

where $\log(\text{L})$ is the log likelihood, $p$ is the number of estimated parameters and $n$ is the number of observations.

* `AIC()` Akaike information criterion
* `BIC()` Bayesian information criterion

Interpretation

* Smaller is better
* BIC usually stronger penalization for numbers of parameters
* Evidence ($\Delta \text{AIC}$, $\Delta \text{BIC}$)
    - 0-2 Weak
    - 2-6 Positive
    - 6-10 Strong
    - >10 Very strong

### Variable selection

#### Backward elimination
```{r, eval=FALSE}
## Manual
fit01 <- lm(Mortality ~ JanTemp + JulyTemp + RelHum + 
	    Rain + Educ + Dens + NonWhite + WhiteCollar +
	    log(Pop) + House + Income + log(HC) +
	    log(NOx) + log(SO2), data=apm) # full model

drop1(fit01, test = "F")
fit02 <- update(fit, .~. – House ) # remove predictor with highest p-value / highest AIC/BIC
drop1(fit02, test = "F")
# ...

## Automatic
fit01 <- lm(Mortality ~ JanTemp + JulyTemp + RelHum +
	    Rain + Educ + Dens + NonWhite + WhiteCollar +
	    log(Pop) + House + Income + log(HC) +
	    log(NOx) + log(SO2), data=apm)
fit02 <- step(fit01, direction = "backward", k = 2) # k defines selection criterion (AIC, BIC)
```

#### Forward selection
```{r, eval=FALSE}
## Manual
fit01 <- lm(Mortality ~ 1, data = apm)
f_full <- lm(Mortality ~ JanTemp + JulyTemp + RelHum +
	      Rain + Educ + Dens + NonWhite + WhiteCollar +
	      log(Pop) + House + Income + log(HC) +
	      log(NOx) + log(SO2), data=apm)
add1(fit01, scope = f_full, test = "F")
fit02 <- update(fit01, .~. + NonWhite ) # add predictor with lowest p-value / lowest AIC/BIC
add1(fit02, scope = f_full, test = "F")
# ...

## Automatic
f_full <- lm(Mortality ~ JanTemp + JulyTemp + RelHum +
		Rain + Educ + Dens + NonWhite + WhiteCollar +
		log(Pop) + House + Income + log(HC) +
		log(NOx) + log(SO2), data=apm)
f_null <- lm(Mortality ~ 1, data = apm)
sc <- list(lower = f_null, upper = f_full)
fit02 <- step(f_null, scope = sc, direction = "forward", k = 2) # k defines selection criterion (AIC, BIC)
```

#### Both directions
```{r, eval=FALSE}
f_full <- lm(Mortality ~ JanTemp + JulyTemp + RelHum +
		Rain + Educ + Dens + NonWhite + WhiteCollar +
		log(Pop) + House + Income + log(HC) +
		log(NOx) + log(SO2), data=apm)
fit02 <- step(f_full, direction = "both", k = 2) # k defines selection criterion (AIC, BIC)
```

#### All possible models
```{r}
f_full <- lm(Mortality ~ JanTemp + JulyTemp + RelHum +
		Rain + Educ + Dens + NonWhite + WhiteCollar +
		log(Pop) + House + Income + log(HC) +
		log(NOx) + log(SO2), data=apm)
fit02 <- leaps::regsubsets(formula(f_full), nbest = 1, dat = apm, nvmax = 14)
plot(fit02) # black predictors are included in the model
```

```{block2, type='rmdcaution'}
`leaps::regsubsets` cannot deal correctly with factor variables and interaction terms
(removes single factor levels instead of entire factor, removes main term even if interaction is still included)
```

#### Select model with lowest predictive MSE
```{r, eval=FALSE}
## Automatic (not recommended)
DAAG::CVlm(data, formula, fold.number, ...)


## Manual (this code only test one possible model)
dat <- apm

# Randomly shuffle the data
dat <- dat[sample(nrow(dat)), ]

# Create 10 equally size folds
f <- 10
folds <- cut(seq(1, nrow(dat)), breaks = f, labels = FALSE)

# Perform 10 fold cross validation
SE <- vector("numeric", f)
for(i in 1:f){
  #Segement your data by fold using the which() function
  testIndexes <- which(folds==i,arr.ind=TRUE)
  testData <- dat[testIndexes, ]
  trainData <- dat[-testIndexes, ]
  
  #Use the test and train data partitions to check one model
  fit <- lm(Mortality ~ NonWhite, data = trainData)
  pred <- predict(fit, newdata = testData)
  SE[i] <- mean((testData$Mortality - pred)^2)
}
mean(SE)
```

### Predictor relevance

significance $\neq$ relevance

```{r, eval=FALSE}
## Standardized coefficients
lelaimpo::calc.relim(fit, type = "betasq", rela = TRUE)

## LMG criterion
lelaimpo::calc.relim(fit, type = "lmg", rela = TRUE)
```


## Shrinkage

See chapter\ \@ref(Computational-Statistics)


## Smoothing

```{r, echo=FALSE}
plot(unique2010$ATM, unique2010$Pax, xlab = "ATM", ylab = "PAX")
polygon(x = seq(19000, 25000, 1),
        y = (dunif(seq(19000, 25000, 1), min = 20000, max = 21000)*6*10^8+par("usr")[3]),
        col = "#f7fcb9")
polygon(x = seq(19000, 25000, 1),
        y = (dnorm(seq(19000, 25000, 1), mean = 23000, sd = 1000*0.37065)*6*10^8+par("usr")[3]),
        col = "#addd8e")

points(unique2010$ATM, unique2010$Pax)

points(ksmooth(unique2010$ATM, unique2010$Pax, kernel = "box", bandwidth = 1000, x.points = 20500),
       cex = 1.5, pch = 16)
points(ksmooth(unique2010$ATM, unique2010$Pax, kernel = "normal", bandwidth = 1000, x.points = 23000),
       cex = 1.5, pch = 16)


segments(x0 = 20750, y0 = 0, y1 = 2100000, lty = 2)
segments(x0 = qnorm(0.75, mean = 23000, sd = 1000*0.37065), y0 = 0, y1 = 2100000, lty = 2)

text(x = c(20750, qnorm(0.75, mean = 23000, sd = 1000*0.37065)),
     y = 2150000,
     labels = "Q 75%")
```

### Running mean (yellow box)

```{r, tidy=FALSE}
fit <- ksmooth(unique2010$ATM,unique2010$Pax,  # Data
               kernel = "box",                 # weighting of points
               bandwidth = 1000,               # spread of weighting function
               n.points = 24,                  # number of points which should be evaluated
               x.points = unique2010$ATM)      # where they should be evaluated
plot(unique2010$ATM, unique2010$Pax)
lines(fit)
```

\begin{align*}
    && 0.25 \cdot \text{bandwith} &= Q_{75\%} \\
    \Rightarrow && \text{bandwith} &= \text{complete width of box}
\end{align*}


### Gaussian kernel estimate (green gaussian)

```{r}
fit <- ksmooth(unique2010$ATM,unique2010$Pax,
		  kernel = "normal",
		  bandwidth = 1000,
		  n.points = 1000)
plot(unique2010$ATM, unique2010$Pax)
lines(fit)
```

\begin{align*}
 && P(X \leq 0.25 \cdot \text{bandwith}) &= 0.75 \\
 \Rightarrow && P\left(Z \leq \frac{0.25 \cdot \text{bandwith}}{\sigma}\right) &= 0.75 \\
 \Rightarrow && \frac{0.25 \cdot \text{bandwith}}{\sigma} &= \Phi^{-1}(0.75) \\
 \Rightarrow && \text{bandwith} &= \frac{\Phi^{-1}(0.75)}{0.25}\sigma
\end{align*}

### LOESS smoother

```{r}
smoo <- loess.smooth(unique2010$ATM,unique2010$Pax,  # Data
                     span = 2/3,                     # smoothing parameter (smaller => smoother)
                     degree = 1,                     # 1: local linear regr, 2: local polynomial regr
                     family = "symmetric")           # "symmetric": robust fitting, "gaussian": least squares fitting

plot(unique2010$ATM, unique2010$Pax)
lines(fit)
```


## GAM

Use package `mgcv` and have a look into the book of Wood.

* `mgcv::s()` smooth spline
* `mgcv::te()` interaction between two predictors
* `mgcv::ti()`
* `mgcv::t2()`
* `splines::ns()` natural spline
* `gam::lo()` loess



```{r}
x <- 1:30
y <- rnorm(30)+10*sin(c(1:30)*0.2)
plot(x, y)
library(mgcv)
fit <- gam(y ~ s(x))
lines(x, fitted(fit))
summary(fit)
```

* `edf` Empirical degrees of freedom
* `Ref.df` Alternative way to calculate df
* `GCV` Generalized cross validation score
* `Deviance explained` Equivalent to $R^2$ 
* `Scale est.` estimate of $\sigma_E^2$

Deviance explained:	Equivalent to R2 (not adjusted)
Scale est.:			estimate of 𝜎_𝐸^2

##### Options {-}

```{r, eval=FALSE}
s(x1, bs = "tp") # tp: thin plate, cr: cubic spline
s(x1, df = 3)    # prespecify the df (instead of determine it by CV)
```

##### Diagnostics {-}
```{r}
fit <- gam(Mortality ~ s(Educ), data = apm)
gam.check(fit)
```

* low `p-value` and `k-index` < 1 indicate that actual fit might need more flexibility
    - increase k-value manually
    - `gam(Mortality ~ s(Educ, k = 50), data = apm)`

##### Visualization {-}

```{r, fig.align='default', fig.show='hold'}
## Partial residual plot
fit_gam <- gam(prestige ~ s(income) + s(education), data = Prestige)
plot(fit_gam, shade = T, residuals = T, main = "GAM Partial Residual Plot", pch = 20)

## 3D plot (only with exactly two s() terms)
vis.gam(fit_gam, theta = 45, phi = 30) # rotation arround vertical (theta) and horizontal (phi) axis

## Interaction spline
fit_gam <- gam(prestige ~ te(income, education), data = Prestige)
plot(fit_gam)
```

##### Model comparison {-}
```{r}
fit_ols <- lm(prestige ~ income + education, data = Prestige)
fit_gam <- gam(prestige ~ s(income) + s(education), data = Prestige)
anova(fit_ols, fit_gam, test = "Chisq")
```

In our case clear evidence for the more complex GAM model.



##### Alternatives {-}

* `smooth.spline()` only one dimensional splines
* package `gam`
* package `earth`


## GLM

### Logistic Regression

##### Fitting {-}

```{r}
fit <- glm(survival ~ log10(weight) + age, data = baby, family=binomial)
```

Possible warning messages of fitting

```
Warning message:
glm.fit: algorithm did not converge
```
Numerical optimization failed, coefficients are not trustworthy 


```
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occured
```

Perfect separation between binary response options, possible to continue… 

##### Inference {-}

* `summary()`
* `drop1()`

Both should yield in similar results. `drop1()` might be better suited for factorial predictors and interactions (drops only terms which can be dropped).


```{r}
summary(fit)
drop1(fit, test = "Chisq")
```

A p-value for a "global F-test" can be manually computed

```{r}
1-pchisq(fit$null.deviance-fit$deviance, df=(fit$df.null-fit$df.res))
```

**CI**

```{r}
## Automatic (slightly different (better) than manual CI)
confint(fit, "log10(weight)")
confint(fit, "(Intercept)")

## Manual
mycoef <- summary(fit)$coefficients
mycoef["(Intercept)","Estimate"] + qnorm(c(0.025, 0.975))*mycoef["(Intercept)","Std. Error"]
```

**Coefficient of determination** $R^2$

```{r}
fit_dev <- fit$deviance
fit_null <- fit$null.deviance
n <- length(baby$weight)
R_2 <- (1-exp((fit_dev-fit_null)/n))/(1-exp(-fit_null/n))
```

##### Visualization {-}

```{r}
## Bernoulli probability
lin_pred <- predict(fit, type="link")       ## linear predictor
bern_prob <- predict(fit, type="response")  ## bernoulli probability
plot(lin_pred, baby$survival, type="n", xlab="linear predictor")
points(lin_pred[baby$survival==0], baby$survival[baby$survival==0], pch = 16, col = "red")
points(lin_pred[baby$survival==1], baby$survival[baby$survival==1], pch = 17)
lines(sort(lin_pred), sort(bern_prob), lty=3)
title("Survival vs. Linear Predictor")

## Predictor space (only if exactly two predictors)
plot(age ~ weight, data=baby, type="n")
points(age ~ weight, subset=(survival==0), data=baby, pch = 16, col = "red")
points(age ~ weight, subset=(survival==1), data=baby, pch = 17)
title("Survival after Premature Birth")
```

##### Diagnostics {-}

Different types of residuals

```{r, eval=FALSE}
resid(fit, type = "pearson")
resid(fit, type = "deviance")
```

Tukey-Anscombe plot should be done manually because we need a non-robust smoother

```{r}
xx <- predict(fit, type="link")
yy <- residuals(fit, type="deviance")
plot(xx, yy, pch=20, main="Tukey-Anscombe Plot")
lines(loess.smooth(xx, yy, family="gaussian"), col="red")
abline(h=0, lty=3, col="grey")
```


### Poisson Regression

##### Fitting {-}

```{r}
fit <- glm(Species ~ log(Area) + log(Elevation) + log(Nearest) +
             I(log(Scruz+0.4)) + log(Adjacent), data = gala, family = poisson)
```

##### Inference {-}

```{r}
summary(fit)
```

##### Diagnostics {-}

Residuals should lie within range $[-2, 2]$, this would not be fulfilled in our example:

```{r}
xx <- predict(fit, type="link")
yy <- resid(fit, type="pearson")
plot(xx, yy, main="Tukey-Anscombe Plot",
     xlab = "Linear Predictor",
     ylab = "Pearson Residuals",
     ylim = c(-10, 10))
smoo <- loess.smooth(xx, yy)
abline(h=0, col="grey")
abline(h = c(-2,2), col = "gray", lty = 2)
lines(smoo, col="red")

```

